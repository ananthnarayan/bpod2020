{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shaanzie/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Clusters')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEYCAYAAAC9Xlb/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de5RcaVnv8e+u7iRd1cmQVNcMGhgcENS+zHFGIRnPTOtMWCIKiMuDryDMCIekZ7wd8X4Xj4rKEhUU1lnpADIgt0dk1AOCYIBJZi1IHMaZ6QuOCzkMMKG701WJTFdVX9J7nz/2JVXVl/SlqtLd+/dZKytde+/3VrX3U2+9tet9vSAIEBGRnS1ztSsgIiKtp2AvIpICCvYiIimgYC8ikgIK9iIiKdB5tSuwCt0mJCKyft5yG7dysOfcuXMbSlcoFJienm5ybbYmtXXnSUs7QW1ttoMHD664T8M4IiIpoGAvIpICCvYiIimgYC8ikgIK9iIiKdC2u3Gcc78AHCW8pXIEeI2ZzbarfBGRNGtLz9459zTgfwHPNbMBoAN4eTvKTrMgCKhUKmx0ZtPNpm+W2noEQUC5XKZcLuP7/pL6LVfn1drh+z7T09P4vp8ct7i4uGSb7/tJuY35rPV5iuv+5JNPMjU1xZNPPlmXXxAEzMzMMDExwczMzJL8fN/n/Pnzdelq6x/vXy6tSDvvs+8Ess65BSAHbOwmelmTIAgYGhpiZGSEgYEBTpw4gect+1uLlqRvltp63HzzzczPz/PAAw8QBAHXXHMNnZ2dSf2AJXVeblvcDt/3OXz4MMVikQMHDnDzzTfz6KOPcvHiRS5dupRsGxkZ4dKlS8zMzAAwODiY5LPW5ykIAo4dO8b999/PwsJCsn3Pnj0MDg4yPDzMsWPHOHnyJACZTIYjR44k+fm+z6FDh5L7tPfs2cOtt97Ko48+SqlUYv/+/XieR7FYXJJWBNoU7M3sCefcm4CvAFXgE2b2icbjnHNDwFCUhkKhsKHyOjs7N5x2u1mpreVymfHxcSYmJujo6CCXy9Hd3b3mfDebvllq6/Hwww/j+z5zc3MAlEolfN9P6gcsqfNy2+J2TE1NUSwW8X2fUqnE6OgoU1NTSdnxtsnJySSwA4yNjSX5rPV5KpfLjI2N1QV6gLm5OcbGxgAYHR1Ntvu+z+joaJJfXNfadKOjo8lzUCqV6t7EatNuZbpW21h+Owpxzh0AXgo8E7gI/K1z7lVm9je1x5nZMDAcPQw2+msz/Sov7En29vayuLhIb28vlUqFarW65nw3m75Zautx0003MT8/z8WLF+t69nH9gCV1Xm5bbTvy+TylUokDBw4wMDCA7/tcuHCBxcXFZFsQBHU9+/7+/iSftT5PQRDQ19fH9PT0kp59f39/ku/k5CQQ9uwHBgbq8svn80nA37NnDwMDAyv27BvTblW6VptrtV/Qeu0Y23PO/RjwQjN7bfT4LuAWM/vpVZIFmi7hylZraxAEVKtVstnshj7ObzZ9s8T1uP7665menk6CeDabZXZ2tq5+y9V5tXbEveJ8Po/neVSrVfbs2cOFCxfqtnV1dSWBM5fL1eWz1uepdvw/Pj6TyST5xWP6XV1dzM7O0t3dXZef7/sUi0W6urqSdEEQJPUHKBaLZLPZJWm3Kl2rzRUF+6s6N85XgFuccznCYZznAw+2qezU8jwvGcq4GumbJa6H53l4nlc3NNFYv+XqvFo7MplM3Ufr+Ljltq00JLLW56m27vv27Vt2/969e1cMCplMhmuvvXZJmtq6Nu4XibXlbhwzOwN8CHiI8LbLDJeHa0REpMXadjeOmb0eeH27yhMRkcv0C1oRkRRQsBcRSQEFexGRFFCwFxFJAQV7EZEUULAXEUkBBXsRkRRQsBcRSQEFexGRFFCwFxFJAQV7EZEUULAXEUkBBXsRkRRQsBcRSQEFexGRFFCwFxFJgXYtOP7twAdrNj0L+F0ze3M7yhcRSbu2BHszewy4CcA51wE8AdzXjrJFRJohXjA+CIJk3eEgCJieniYIArq7u+nu7iYIAorFInv27GF2dpZsNgvA4uJisgj89PQ0vu8vWV+5cTH7ZmrbsoQ1ng/8p5k9fhXKFhFZtyAIOHbsGKdOnWJhYYHdu3dz66238vDDD1MqlYBwQfjbb7+dRx55hGKxuGw++XweIEkT8zyP3bt3Mzg4yIkTJ1oS8K9GsH858P7ldjjnhoAhADOjUChsqIDOzs4Np91u1NadJy3thO3T1nK5zNjYGPPz8wDMzc0xOjpaF7R932dkZGRJIK+10r4gCJibm2NsbIxcLkd3d3dzG0Cbg71zbjfww8BvLLffzIaB4ehhMD09vaFyCoUCG0273aitO09a2gnbp61BENDX10exWEx69gMDA0t69jfeeOOmevb9/f1UKhWq1eqG6nnw4MEV97W7Z/+DwENmNtnmckVENszzPN7+9rdvasy+djw+DWP2r2CFIRwRka3M87wlwyue53Hdddct2XbttdcCcM011yTbaz/FNKZph7bdZ++c6wa+H/hwu8oUEZFQ23r2ZlYGetpVnoiIXKZf0IqIpICCvYhICijYi4ikgIK9iEgKKNiLiKSAgr2ISAoo2IuIpICCvYhICijYi4ikgIK9iEgKKNiLiKSAgr2ISAoo2IuIpICCvYhICijYi4ikgIK9iEgKtG3xEufcfuDtwAAQAP/TzD7brvJFRNKsnWvQvgX4uJm9zDm3G8i1ohDf95mamgLC1d6DIKBcLlMul+sWCgaoVqt0dXUxNzdHT08PmUz4QScIAqrVKtlstmWL/4q0iu/7yYLX8bkNUCqVyOfzZDIZfN9P1kMtFArJuV8rvnYqlQq5XI7u7m48z1t2O0ClUsH3fWZnZ5PrqfZaio+By4tv+77P+fPnyWazK+afy+WoVqtJunhB766uLjKZTHI9r1TX2ms5brfv+2Qymbq2r3Rs7WLi2zketCXYO+eeAnwv8GoAM5sH5ptdju/7HD58mGKxSD6f58yZMwwNDXHy5Mkrpi0UCpw9exbP8xgaGmJkZISBgQFOnDixrV9gSRff9zl06FASyIEkwMfB/nOf+1xyncDlc7824AdBwNGjR5NrJ5PJcOTIEYaHhzl27Fjd9jvuuAOAU6dOsbCwkOR55swZ7rnnHkZGRujv7wfggQceAGBwcJDjx4/z7Gc/m6mpqRXz9zyPfD7PzMwMALfeeiuPPPJIUvc9e/Zw2223EQQBn/rUp5bU9e67706u5ePHj3Po0KEkbW3bG6/7xmPjPLdzPGhXz/6ZwHngr51z3wl8Hvj5aF3ahHNuCBgCMDMKhcK6CpmamqJYLOL7PqVSiWq1yujo6JrSlkolIOw5jI+PMzExQUdHR13PZSvq7Oxc9/O0XaWlrZtpZ3wN1CqVSklvvlQqUSqV6o6Jz/3aMsvlct214/t+8rhx+8jICJ7nJYE+zrNarSbXUtxTnpubA2BsbIxqtcr58+dXzT8IAkqlEkEQJPvi+gLMzc0xOjqa7G/Mq/Zarlaryz43sPS6bzw2znMz8eBqn7/tCvadwHcBP2dmZ5xzbwF+Hfid2oPMbBgYjh4Gtb2Ttcrn80kPJpvN0t/fz+Tk5JrSQfgxs7e3l8XFRXp7e6lUKslHyK2oUCiwkedpO0pLWzfbznw+XxeoGnv28b840MXnfm2ZQRDUXTuZTIaBgQGAJdtvvPFGIAycccCPr7/4Wurr6wPg4sWLSR7ZbJZCocD58+dXzL+xZz8wMLCkZz8wMLCkZx/nVXstZ7PZunbXtr3xum88Ns5zM/GgHefvwYMHV9zn1b4jtopz7puAz5nZDdHjQeDXzexFqyQLzp07t+6yfN9P/k7DmH1aAiCkp62bbed2GrPP5/M89thjqRizb2OwX7aSbQn2AM6508BRM3vMOfd7QLeZ/coqSTYU7CE9QQHU1p0oLe0EtbXZVgv27bwb5+eA90Z34nwJeE0byxYRSbW2BXszexh4brvKExGRy/QLWhGRFFCwFxFJAQV7EZEUULAXEUkBBXsRkRRQsBcRSQEFexGRFFCwFxFJAQV7EZEUULAXEUkBBXsRkRRQsBcRSQEFexGRFFCwFxFJAQV7EZEUaNt89s65LwNPAovAJTPT3PYiIm3SzpWqAO4ws5avQRYEATMzM5TLZTzPW3GNzZjv+5RKJfbv35+sZB+vaxmvmZnNZpmdna1bczNew3Z2djb5v3Hd2jjveO1PkXZabg3Y2rWY43M1Xrc2m80uWfMVLp/r8YLbjWs2N64t23jeL7euc7zWbFyfxvo2riEb17+2nJXKb9wWr1tbu9Zt7XN0pfrvBO0O9i0XBAHOOT72sY8l2wqFAmfPnl32RfN9n8OHD3P+/Pk15Z/JZLjjjjvwPI+RkREWFxfp6OhgcXGRzs5OBgYGOHHiRHLCHD58mGKxSD6fX7EOIq0QBAFDQ0OMjIzQ398PwOnTp1lYWGD37t0MDg5y4sQJgiDg0KFDTE9PJ4uAz8zMAHDbbbcl5/rCwgIXLlwgCILkmvI8j2PHjnH69GkABgcHOX78OLfcckty3p85c4Z77rmHkZGR5PoAGBoaYnx8nN7e3rpt8XHDw8PcfffddfV/4IEHknLiNI3lDw8PMzQ0lGy79dZbeeSRRygWi2QyGY4cOZJco0EQXLH+O+W6bWewD4BPOOcC4LiZDTce4JwbAoYAzIxCobDuQsrlMg899FDdtlKpBLBsflNTUxSLxTXn7/s+IyMjdHR0MDk5SSaTSVaq932fjo6OpPcf5x33Elaqw2Z0dnY2Pc+tKi1tbVY7y+Uy4+PjTExMJJ2P+fl5AObm5hgbGyOXy1Eul5NrIAiC5NMtwOjoKJlMhsnJybq84/M5l8sxNjbG3NwcAGNjY1Sr1brzvlqtJvWIrw+A8fFxnnjiiSSfeFt8XO3juP615cRpGstv3DY6OprU1/d9RkdHk2u0XC5fsf7QnOv2ap+/7Qz2t5nZE86564BPOuf+3cxO1R4QvQHEbwLBRlZiD4KAm2++mY9//OPJtnw+D7Diyu75fH7Nq75nMhluvPHG5ORr7Nn39vYmH3XjvOOPg6vVYaPasWL9VpGWtjarnUEQ0Nvby+LiIn19fQBcuHAh6dn39/cnwxf5fJ5isbikZz8wMJCc67U9+/h8rlQq9PX1JUGxv7+fbDZbd95ns9mkHvH1AdDb25v8X7stPq72cVz/ixcvJuXEaRrLb9w2MDBQ17MfGBhIrtEgCK5Yf2jOdduO8/fgwYMr7vPid/B2cs79HjBjZm9a5bDg3LlzG8q/p6eHxx9/PBVj9mkJgJCetjaznVt9zD6Xy1GpVFIxZt/GYO8tt68twd451w1kzOzJ6O9PAr9vZh9fJdmGg31aggKorTtRWtoJamuzrRbs1zyM45zrA4pmNumc2wv8CuADf2pmlSskfypwn3MuLvN9Vwj0IiLSROsZs38/4IBJ4E3AtwOzwHHgztUSmtmXgO/cYB1FRGST1hPsbzCzx5xzHvCjQB9QBf5fS2omIiJNs55vHmadc/uAQ8BXoh9HzQFdLamZiIg0zXp69u8DPg3sBd4abfsu1LMXEdny1tyzN7NfAH4T+Ckzi4O9D/xCKyomIiLNs6aevXOuA/gPoM/M5uLtZvZgqyomIiLNs6aevZktEs5WqfF5EZFtaD1j9m8GzDn3R8DXCOe6AZJbK0VEZItaT7CPx+m/v2F7AHQ0pzoiItIKaw72Zrb95/gUEUmpdQdw59z1zrlbWlEZERFpjfXMjfMMwikTbiIcutnrnHsZ8EIzO9qi+omISBOsp2d/HPgosA9YiLZ9kqVj+CIissWsJ9gfAv7EzHyiO3HM7L+Ap7SiYiIi0jzrCfaTwLNrN0TTHn+lqTUSEZGmW0+wfxPwEefca4BO59wrgA8Cb2xJzUREpGnWMzfOOwkXLPkx4KvAXcDvmNl7W1Q3ERFpkvXcjXPYzP4B+IeG7YfM7Owa8+gAHgSeMLMXr6umIiKyYesZxvnkCtvXs7zgzwNfWMfxIiJbShAEzMzMMDU1xczMDEEQEAQB5XKZJ598kvPnz+P7fnJsvMi77/ucP38+SVObX7lcZmZmZkn6Zrpiz945lyFcwNaLVqmqXcz2W4FLaynIOfd04EXAG4BfXH9VRUSuriAIOHr0KCdPngQgk8lwxx13AHDq1CkWFsK70guFAmfOnOGee+5hZGSEvr4+RkdHmZqaIpPJcOTIEU6cOAHAsWPHOHXqFPPz80k5hUKBs2fPksk0b+KCtQzjXOLypGeNgd0nDN5r8WbgVwnv01+Wc24IGAIwMwqFwhqzrtfZ2bnhtNuN2rrzpKWdsP3aWi6XGR0dTR77vs/IyAie5yWBHqBUKlGtVhkfH2diYoIgCDh//nySZnR0lFwuB8DY2FhdoI/TA019btYS7J9J2Ju/H/jemu0BcN7MqlfKwDn3YmDKzD7vnLt9pePMbBgYjvOfnp5eQ/WWKhQKbDTtdqO27jxpaSdsv7YGQUB/fz+Tk5NA2LO/8cYbgTBAxwE/n8+TzWbp7e1lcXGRvr6+ZBgnk8kwMDBApVIBoK+vj2KxWBfw8/k8wLqfm4MHD664z6sdO1oP51wW8GsXM1nl2D8G7iT8ZNAFXAN82MxetUqy4Ny5cxuq23Y7gTZDbd150tJO2J5tjcfYK5UKuVyO7u5uACqVCr7vMzs7S09PD5lMhiAIqFarZLNZenp6eOyxx8hms3R3d+N5XpJfPK4fBEFd+vWKgr233L41B3vn3JsAM7OzzrkXAR8i7N3/uJn937VWJurZ//Ia7sZRsF8DtXXnSUs7QW1tttWC/XreOl4JxINVvwu8Cvhh4I82UzkREWm99SxekjOzinOuB3iWmf0dgHPuW9ZToJl9BvjMetKIiMjmrCfY/4dz7pWE8+N8EsA5VwCu+AWtiIhcXesJ9j8NvAWYB14bbfsB4BPNrpSIiDTXepYl/Ffgvzdsey+guXFERLa49cyNc2SlfWb2qeZUR0REWmE9wzjvaHh8LbAb+BrwrKbVSEREmm49wzjPrH0czWD528CTza6UiIg014Zn2TGzRcJ5cX61edUREZFW2OyUat9POBmaiIhsYev5gvarXJ79EiBHOM/NTze7UiIi0lzr+YK2cdKyMvAfZvaNJtZHRERaYD1f0N7fyoqIiEjrrBrsnXPvoX7oZllmdlfTaiQiIk13pZ79F9tSCxERaalVg72Z/W/n3K3AD5vZrzXud869EbivVZUTEZHmWMutl78JnFph36eB32pedUREpBXWEuxvAj6+wr5/Ab67edUREZFWWMvdONcQzoGz3Lz1u4B9V8rAOddF+OlgT1Tmh8zs9euop4iIbMJagv2/Ay8A/mGZfS+I9l/JHHDEzGacc7uAB5xzHzOzz629qrIW8WLI5XIZgFwuB5CsZJ/NZqlWqyv+HQQBmUyGfD7PhQsX6OrqwvM8PM8jl8sliyS3sz3VapU9e/YwMTFBsViku7ubbDZLqVSiq6urrn3LtXd2dpZsNrtke7VaxfM8enp6qFarVCoVstksmUyGrq6uuvxnZ2fJ5/NUq9Xkua3NI5/PUyqVCIKg7rn0PG9JWXNzc2Sz2brnMl50GqCnp2fJ8+D7PqVSiXw+v2Qh6tpFrdv9+sj2sZZg/xfA8Wjis783M985lwF+BHgb8ItXysDMAmAmergr+re2lc5lzYIg4OjRo5w8ebKp+Xqex+7duxkcHOTEiRNtCyhBEDA0NMQjjzxCqVTi0qVLyb5du3axsLDQlHI6OzuX5A0syb/xuI3YvXs3+XyegYGB5LkMgoBjx45x+vRpAJ7//Ofztre9LXmefd/n8OHDFItF8vk8Z8+eTQJ+/ByNjIzU5SnS6IrB3sze55z7JuBeYI9zbhooEPbWX29m719LQdGbxecJlzV8m5mdWeaYIWAoKpdCobDmhtTq7OzccNrtprat5XKZ0dHRK6RYvyAImJubY2xsjFwuR3d3d9PLWE65XGZ8fJypqakl+5oV6IElAXylvDcb6AHm5+eZmJigo6MjeS7L5TJjY2PMzc0B8G//9m91z/PU1BTFYjHp3QN1r/n4+PiSPLeLtF6rV6X8tRxkZn/unHs78D1AD1AEPrueqRKiWTJvcs7tB+5zzg2Y2WjDMcPAcPQwmJ6eXmv2dQqFAhtNu93UtjUIAvr7+5mcnGxqGXHPvr+/n0qlkgz9tFoQBPT29rKwsECxWGRxcTHZ14xedqyjo6Mu7127dhEEwZL8G4/biF27dtHT00Nvb2/yXAZBQF9fXxLIb7755iXPczxMlM/nAepe897eXhYXF+vy3C7Seq22ysGDB1fc5wVB+0dTnHO/C1TM7E2rHBacO3duQ/mn+QTayWP2QRCkYsz+Gc94BsVise552Klj9mm+VlshCvbLngRtCfbOuWuBBTO76JzLEi5S/kYz+8gqyRTs10Bt3XnS0k5QW5tttWC/nlkvN+ObgXujcfsMYFcI9CIi0kRtCfZm9ihwczvKEhGRpTa7UpWIiGwDCvYiIimgYC8ikgIK9iIiKaBgLyKSAgr2IiIpoGAvIpICCvYiIimgYC8ikgIK9iIiKaBgLyKSAgr2IiIpoGAvIpICCvYiIimgYC8ikgJtmc/eOXc98G7gqUAADJvZW9pRtoiItG+lqkvAL5nZQ865fcDnnXOfNLPxNpW/Jfi+z/T0dN06pcCKa5bW7lvpuIWFBUqlUt06p4VCYck6pZsRr2tbqVTo6uracF03ctzs7Cy5XI7u7u6mtUckjdq1UtXXga9Hfz/pnPsC8DQgNcHe932e97znLVlIuhUKhQJnz55tSsAPgoCjR49y8uTJJtRsYzKZDEeOHOG+++67anUQ2e7a1bNPOOduIFyi8Mwy+4aAIQAzo1AobKiMzs7ODadtlampqbYEeoBSqQTQlOegXC4zOjq66Xw2w/d9RkdHmZ+f33KvaytsxfO3VdTWNpbfzsKcc3uBvwNeZ2bfaNxvZsPAcPQw2OhK7Ft1xfp8Pp8E4laXAzTlOQiCgP7+fiYnJzed10ZlMhkGBgbYvXv3lnxdm22rnr+toLY218GDB1fc17Zg75zbRRjo32tmH25XuVtFJpPhwQcfbPqYfTabbemYved5vOMd79gSY/ae5zWlTSJp5AVB0PJCnHMecC9QMrPXrTFZcO7cuQ2Vp97CzpSWtqalnaC2NlvUs1+2V9Sunv2twJ3AiHPu4Wjbb5rZP7WpfBGRVGvX3TgPsMK7jYiItJ5+QSsikgIK9iIiKaBgLyKSAgr2IiIpoGAvIpICCvYiIimgYC8ikgIK9iIiKaBgLyKSAgr2IiIpoGAvIpICCvYiIimgYC8ikgIK9iIiKaBgLyKSAgr2IiIp0JbFS5xz7wReDEyZ2UA7yhQRkcvatSzhu4C3Au9uU3mrCoKAarVKNptNFrFebtty6SqVCr7vU61W6erqYm5ujgMHDnDhwgW6uroAqFQqQLhgdu3+bDarhbOlpRrP4/ichcsLuHd1dTE7O0s2mwWgXC5TrVbp6ekBoFgsks1myeVydcfF+eRyubq848Xi4+2+71MqlThw4ACzs7P4vs/s7Cz5fJ7Z2VmCIEjquWfPnuRxvD5rT08Pc3NzdeXWLki/Ut0zmUxdnQA8z6Orq4sLFy6Qz+fJZC4PZgRBQLlcplKpkMvlyOVyVCqV5HHtter7fvK8bNdruF3LEp5yzt3QjrKuJAgChoaGGBkZYWBggBMnTgAs2db4YgZBwLFjx7j//vtZWFjYUNmZTIYjR44sm7/IZjWe28PDwwwNDXH69GmCIOCaa66ho6ODxcVFOjs76e/vJwgCPv3pTxMEAfl8Hs/zKBaLeJ5HT09PchzAAw88AMDg4GCS96lTp1hYWGD37t0MDg5y/PhxbrnlFqanp+nsDMNLfL3s2rULz/OYn59P6pzJZLj99tt5+OGHKZVKAOzevZt8Pp+Ue/r0aRYWFti1axfXXHPNsnUvFAqcOXOGu+++m1OnTjE/P4/neXR2duJ5HpcuXSKfz3P27NnkTeHo0aOcPHkSCN8UDhw4kNSh9loNgoBDhw4xPT29ra/hdvXs18Q5NwQMAZgZhUJhQ/l0dnaumLZcLjM+Ps7ExAQdHR3kcjmAJdu6u7uXpBsbG9twoIewdzA6Orps/hu1Wlt3mrS0daPtbDy3AcbGxpibmwOgVCrh+z6ZTAbf9/E8j8XFxaQXXCqV6j7pxsfHvfU4n7GxseT/OHDPzc0xNjZGtVqlWCwSBMGSa2W5a8f3fUZGRpIgCzA/P8/ExERSblzG/Px8XZ0a616tVuvq1FiHuIxCoUC5XGZ0dDTZF7e3tl7xtVoulykWi0u2r/cavtrn75YK9mY2DAxHD4P4Y916FQoFVkobBAG9vb0sLi7S29ubfDRt3FatVpek6+vrY3p6elM9+4GBgWXz36jV2rrTpKWtG21n47kN0NfXR6lUWrZn39fXt2rPPp/PJ8cBXLx4ESDpcff19VEsFpOefX9/P9lslnw+T7FYpKOjA8/zkuuls7OTTCazpGd/44031vXsd+3aRU9PT1LuhQsXlvTsl6t7NptN6lTbswdYXFwkn88DMD09TRAE9Pf3Mzk5CSzfs4+vVSBp02au4XacvwcPHlxxnxe/M7ZaNIzzkXV8QRucO3duQ2Vd6UndSWP2aQmAkJ62bqad223M/vrrr0+27fQx+zYG+2Urt6V69u0Sn5hX2rZcuvij2759++r2XXvttcnfjfsa94u0SuN5XHvOAsm+2mP27t3L3r17k8e152rtcY3DFo15xzKZTDJc0Xi91B6/b98+9u3bx9zcHJ7ncd11162p3NXqvlKdlhs+8TxvSfrGx7Vt2u7XcFvus3fOvR/4LPDtzrmvOede245yRUQk1K67cV7RjnJERGR5+gWtiEgKKNiLiKSAgr2ISAoo2IuIpICCvYhICijYi4ikgIK9iEgKKNiLiKSAgr2ISAoo2IuIpICCvYhICijYi4ikgIK9iEgKKNiLiKSAgr2ISAoo2IuIpEDbliV0zr0QeAvQAbzdzP6kXWWLiKRdu5Yl7ADeBvwg0Ae8wjnX146yZf0aF22W1giCgJmZGRvFS0oAAAxwSURBVCYnJ5mamsL3/WSf7/tMTU0xOTnJN77xDc6fP5/sj9NNTU0xMzOD7/uUy2XK5bJeM1lRu3r2h4AvmtmXAJxzHwBeCoy3qXxZoyAIGBoaYmRkhIGBAU6cOIHnLbtYvWxCEAQcPXqUkydPJtsKhQJnz57F932e97znUSwW69IUCgXOnDnD0NBQks7zPPL5PDMzMwAMDg7qNZNltSvYPw34as3jrwGHGw9yzg0BQwBmtuyK8GvR2dm54bTbTbPbWi6XGR8fZ2Jigo6ODnK5HN3d3U3LfzN20utaLpcZHR2t21YqlQC4ePHikkAf769Wq3XpgiCgVColPfqxsbEt9ZpdyU56Ta/kare1bWP2a2Fmw8Bw9DCYnp7eUD6FQoGNpt1umt3WIAjo7e1lcXGR3t5eKpUK1Wq1aflvxk56XYMgoL+/n8nJyWRbPp8HYP/+/eTz+ST41+7PZrN16Rp79v39/VvqNbuSnfSaXkk72nrw4MEV97Ur2D8BXF/z+OnRNtliPM/jxIkTVKtVstmshgNaxPM83vGOdyRj7Z7nUSgUyGQyZDIZHnzwQaanpwmCgGw2y9zcHD09PWQymSRdpVIhl8uRy+WS4J7L5fSaybLaFez/FXiOc+6ZhEH+5cBPtKlsWSfP88jlcle7Gjue53ns3buXvXv3LtmXyWS47rrr1pxuuwzbyNXTlrtxzOwS8LPAPwNfCDfZWDvKFhGRNo7Zm9k/Af/UrvJEROQy/YJWRCQFFOxFRFJAwV5EJAUU7EVEUsDbwnNpbNmKiYhsYcv+0GIr9+y9jf5zzn1+M+m30z+1def9S0s71daW/VvWVg72IiLSJAr2IiIpsFOD/fCVD9kx1NadJy3tBLW1bbbyF7QiItIkO7VnLyIiNRTsRURSYEstXrJZO21Rc+fcO4EXA1NmNhBtywMfBG4Avgw4M7vgnPMI2/5DQAV4tZk9dDXqvRHOueuBdwNPJfyNxbCZvWUnttc51wWcAvYQXoMfMrPXR1OAfwDoAT4P3Glm8865PYTPzXcDReDHzezLV6XyGxCtQf0g8ISZvXinthPAOfdl4ElgEbhkZs/dKufwjunZ79BFzd8FvLBh268DJ83sOcDJ6DGE7X5O9G8I+D9tqmOzXAJ+ycz6gFuAn4lev53Y3jngiJl9J3AT8ELn3C3AG4G/MLNnAxeA10bHvxa4EG3/i+i47eTnCac2j+3UdsbuMLObzOy50eMtcQ7vmGBPzaLmZjZP2HN46VWu06aY2Smg1LD5pcC90d/3Aj9Ss/3dZhaY2eeA/c65b25PTTfPzL4e92rM7EnC4PA0dmB7ozrPRA93Rf8C4AjwoWh7Y1vj5+BDwPOjXuGW55x7OvAi4O3RY48d2M4r2BLn8E4K9sstav60q1SXVnqqmX09+nuCcNgDdlD7nXM3ADcDZ9ih7XXOdTjnHgamgE8C/wlcjBb6gfr2JG2N9v8X4RDIdvBm4FcBP3rcw85sZywAPuGc+7xzbijatiXO4Z0U7FPHzAJ22BxCzrm9wN8BrzOzb9Tu20ntNbNFM7uJcD3mQ8B3XOUqNZ1zLv6+6fNXuy5tdJuZfRfhEM3POOe+t3bn1TyHd1KwT8ui5pPxR73o/6lo+7Zvv3NuF2Ggf6+ZfTjavGPbC2BmF4FPA99D+DE+vmmitj1JW6P9TyH8AnOruxX44ehLyw8QDt+8hZ3XzoSZPRH9PwXcR/hGviXO4Z0U7JNFzZ1zuwkXNf/Hq1ynVvhH4Cejv38S+Iea7Xc557zoy77/qvnouOVFY7PvAL5gZn9es2vHtdc5d61zbn/0dxb4fsLvKD4NvCw6rLGt8XPwMuBTUQ9xSzOz3zCzp5vZDYTX46fM7JXssHbGnHPdzrl98d/AC4BRtsg5vGNuvTSzS865eFHzDuCd231Rc+fc+4HbgYJz7mvA64E/Acw591rgccBFh/8T4S1cXyS8jes1ba/w5twK3AmMRGPZAL/JzmzvNwP3RneQZQAzs48458aBDzjn/hD4N8I3P6L/3+Oc+yLhF/YvvxqVbqJfY2e286nAfc45CGPr+8zs4865f2ULnMOaLkFEJAV20jCOiIisQMFeRCQFFOxFRFJAwV5EJAUU7EVEUkDBXlLJOTfmnLu9RXlf65z79+ge+tWOe4lz7oOtqINII916KTuSc26m5mGOcKbJxejx3Wb23haW/WfA+bVMse2cGwV+wswebVV9RGAH/ahKpJaZ7Y3/jn6uf9TM/qXV5UZzsv8k4dTFa/F+wultf7ZllRJBwV5SqvYNwDn3e0A/Ye//pYQLTPyP6N8vRNtfa2afiNI+Bfhzwl8/+sBfA683s0XgMOGsjl+rKevVwO8C1wLTwG/XfLL4DPA3KNhLi2nMXiT0EuA9wAHCn/D/M+H18TTg94HjNce+i3CxlWcTTsX8AuBotO9G4LH4wGiOlL8EftDM9gH/HXi4Jq8vADc4565peotEaqhnLxI6bWb/DOCc+1vgR4E/MbNF59wHgOFo8rI9hD36/WZWBcrOub8gHIo5DuwnXJaulg8MOOe+Ek10VTvZVXzsfuAbiLSIevYiocmav6vAdDQsEz8G2At8C+HKUl93zl10zl0kDPLXRcdcAPbFGZlZGfhx4J4ozUedc7Vz18fHXmxmY0QaqWcvsj5fJRzDL9SstlTrUcJx/kT0ieGfo1sx/xA4AQxGu3uBLzcu1CLSbAr2IutgZl93zn0C+DPn3O8AM8Azgaeb2f3AWcLFOZ5mZk84555KuID6vxB+Qpjh8hJ9AN8HfKytjZBU0jCOyPrdBewGxgmHbT5EOEc90WL37wJeFR2bAX4ROEc4R/v3AT9Vk9crqP/yV6Ql9KMqkSZzzl0LnAZujr7EXem4lwB3mplb6RiRZlGwFxFJAQ3jiIikgIK9iEgKKNiLiKSAbr2ULck59wzCu12eUvPjJhHZIH1BK1tGO2enbKZoorOjZnbb1a6LyEo0jCOyCc65ln06bmXekj7q2cuW4Jx7D/BKLi8y8vvAG4FdZnbJOfcZwnvXjwD/Dfgs4aIf0865jwIfN7O/qsnvUcJph+9boTyPcJriVwJdwOPAK8xsNJrC+K+AHwQqhNMb/JGZ+VEv/hjhL2XvIpyi+EWE8+VUgUtmtn+Vdr4LmAW+lfCXtQ8Bd5nZ49H+gHC649cBnWb2TOfciwmnWbiBcGjrnnixk+jT0HHgTsIfdv098FNmNrvysy1ppJ69bAlmdifwFeAl0cIjtsxhPwG8hnDSsd3AL0fb7+XyL1Zxzn0n4dTEH12lyBcA3wt8G/AUwAHFaN9fRdueRfiL17uicmOHgS8BT43KvQf4rJntXS3Q13gl8AdAgXC648ZVs34kKqPPOXcz8E7gbqCHMLD/Y7RISm1+P0D4BvJtwG+voQ6SMgr2sp38tZn9R/SrVOPyalD/CHybc+450eM7gQ9GUxesZIFwxsnvADwz+0I0700H8HLgN8zsSTP7MvBnUZ6xc2b2V2Z2abVfyK7io2Z2yszmgN8Cvsc5d33N/j82s1KU9xBw3MzOmNmimd1L+Onnlprj32pmXzWzEvAGwikYROoo2Mt2MlHzd4VwymGiIYsPAq9yzmUIg917VsvIzD4FvBV4GzDlnBuOFhApEA7JPF5z+OOEnxRiX91kO5L0ZjZDOGfOwRXy/xbgl+LplKMpla9f5fjHG/aJALr1UraWzXyBdC9hgH8AqJjZZ6+UwMz+EvhL59x1hJ8UfgX4PcJe/7cQjo8DPAN4YpV6rrfeSS/eObcXyBNOlLZcfl8F3mBmb1hLfoR1PbfSgZJeCvaylUwSjpOvm5l91jnnEw65rNqrB3DOPY/wk+1DQJnwS1M/WpnKgDc45+4iDMS/CLzpCvV+unNu9xWGjmI/5Jy7jfBL3j8APmdmK31aOAHc55z7l+j4HHA7cMrM4lWufsY59xHCTzu/RfgpR6SOhnFkK/lj4LejoYqXbSD9uwnXgP2bNRx7DWEgvUA49FEE/jTa93OEbwBfIvyk8D7CL0lX8ilgDJhwzk2voez3Aa8nHL75bmq+XG5kZg8S3v3z1qiuXwRevUx+n4jq+5+Ed+6I1NGtl7JjRD3xoa3846bo1suvmVlT7pjZrj9Ek/ZTz152BOdcDvhpYPhq10VkK9KYvWx7zrkfAD5MuPTf+2q2D7LCkn/Rvfytqs8Y4Re8je5uVZkiV6JhHBGRFNAwjohICijYi4ikgIK9iEgKKNiLiKSAgr2ISAr8f7Rh2jygQ+hyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/HiBench/csvs/3.2.1/terasort_run.csv\")\n",
    "data = data.drop(['Unnamed: 0'], axis = 1)\n",
    "data = data.dropna()\n",
    "# data \n",
    "IPC = list()\n",
    "for i in data.iterrows():\n",
    "    if(int(i[1][5]) == 0):\n",
    "        IPC.append(None)\n",
    "    else:\n",
    "        if(int(i[1][1]) / int(i[1][5]) < -0.4 or int(i[1][1]) / int(i[1][5]) > 1):\n",
    "            IPC.append(None)\n",
    "        else:\n",
    "            IPC.append(int(i[1][1]) / int(i[1][5]))\n",
    "#     print(i[1][4])\n",
    "\n",
    "data['IPC'] = IPC\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "# Create x, where x the 'scores' column's values as floats\n",
    "x = data.values.astype(float)\n",
    "\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "data = pd.DataFrame(x_scaled)\n",
    "\n",
    "# sse = []\n",
    "# list_k = list(range(1, 20))\n",
    "\n",
    "# for k in list_k:\n",
    "#     km = KMeans(n_clusters=k)\n",
    "#     km.fit(data)\n",
    "#     sse.append(km.inertia_)\n",
    "\n",
    "# # Plot sse against k\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.plot(list_k, sse, '-o')\n",
    "# plt.xlabel(r'Number of clusters *k*')\n",
    "# plt.ylabel('Sum of squared distance');\n",
    "\n",
    "# Number of clusters\n",
    "kmeans = KMeans(n_clusters=9)\n",
    "# Fitting the input data\n",
    "kmeans = kmeans.fit(data)\n",
    "# Getting the cluster labels\n",
    "labels = kmeans.predict(data)\n",
    "# Centroid values\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "arr = []\n",
    "for i in range(len(labels)):\n",
    "    arr.append(i)\n",
    "# print(arr)\n",
    "plt.scatter(arr, labels, c='#050505', s=7)\n",
    "plt.xlabel(\"Time(s)\\ntiny_sort_prep\")\n",
    "plt.ylabel(\"Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>Current Cluster</th>\n",
       "      <th>Next Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052351</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>0.007387</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122578</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.184414</td>\n",
       "      <td>0.199815</td>\n",
       "      <td>0.295166</td>\n",
       "      <td>0.410972</td>\n",
       "      <td>0.415218</td>\n",
       "      <td>0.164890</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417736</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.149674</td>\n",
       "      <td>0.164176</td>\n",
       "      <td>0.194539</td>\n",
       "      <td>0.401805</td>\n",
       "      <td>0.534092</td>\n",
       "      <td>0.143095</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389543</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.290563</td>\n",
       "      <td>0.291288</td>\n",
       "      <td>0.529605</td>\n",
       "      <td>0.725636</td>\n",
       "      <td>0.692073</td>\n",
       "      <td>0.256825</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385361</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.214896</td>\n",
       "      <td>0.211615</td>\n",
       "      <td>0.190894</td>\n",
       "      <td>0.136036</td>\n",
       "      <td>0.272686</td>\n",
       "      <td>0.119368</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.656963</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0.056615</td>\n",
       "      <td>0.059945</td>\n",
       "      <td>0.060155</td>\n",
       "      <td>0.040176</td>\n",
       "      <td>0.125481</td>\n",
       "      <td>0.032763</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.660491</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.006295</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.188830</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>0.049597</td>\n",
       "      <td>0.051149</td>\n",
       "      <td>0.121193</td>\n",
       "      <td>0.099192</td>\n",
       "      <td>0.051149</td>\n",
       "      <td>0.042415</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.725830</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.406297</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>0.116715</td>\n",
       "      <td>0.099770</td>\n",
       "      <td>0.048171</td>\n",
       "      <td>0.081230</td>\n",
       "      <td>0.181439</td>\n",
       "      <td>0.076565</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.725830</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.452888</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.000492  0.000480  0.002895  0.006325  0.007090  0.001951  0.000051   \n",
       "1    0.001699  0.001526  0.003426  0.004929  0.007387  0.002809  0.000043   \n",
       "2    0.184414  0.199815  0.295166  0.410972  0.415218  0.164890  0.000997   \n",
       "3    0.149674  0.164176  0.194539  0.401805  0.534092  0.143095  0.001172   \n",
       "4    0.290563  0.291288  0.529605  0.725636  0.692073  0.256825  0.001425   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "513  0.214896  0.211615  0.190894  0.136036  0.272686  0.119368  0.000312   \n",
       "514  0.056615  0.059945  0.060155  0.040176  0.125481  0.032763  0.000098   \n",
       "515  0.002819  0.002917  0.006295  0.007061  0.005291  0.003840  0.000523   \n",
       "516  0.049597  0.051149  0.121193  0.099192  0.051149  0.042415  0.001036   \n",
       "517  0.116715  0.099770  0.048171  0.081230  0.181439  0.076565  0.001124   \n",
       "\n",
       "            7         8         9        10        11  Current Cluster  \\\n",
       "0    0.000000  0.000000  0.000000  0.000000  0.052351                6   \n",
       "1    0.000013  0.000000  0.000000  0.000000  0.122578                6   \n",
       "2    0.000877  0.012987  0.000000  0.000000  0.417736                5   \n",
       "3    0.000990  0.066378  0.000000  0.000000  0.389543                5   \n",
       "4    0.001473  0.066378  0.000000  0.000000  0.385361                5   \n",
       "..        ...       ...       ...       ...       ...              ...   \n",
       "513  0.000243  0.722944  0.957532  0.096714  0.656963                7   \n",
       "514  0.001036  0.727273  0.957532  0.096714  0.660491                4   \n",
       "515  0.001621  0.727273  0.957532  0.096714  0.188830                4   \n",
       "516  0.001726  0.725830  0.957532  0.096714  0.406297                4   \n",
       "517  0.000139  0.725830  0.957532  0.096714  0.452888                4   \n",
       "\n",
       "     Next Cluster  \n",
       "0               6  \n",
       "1               5  \n",
       "2               5  \n",
       "3               5  \n",
       "4               5  \n",
       "..            ...  \n",
       "513             4  \n",
       "514             4  \n",
       "515             4  \n",
       "516             4  \n",
       "517             4  \n",
       "\n",
       "[518 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Current Cluster'] = labels\n",
    "to_cluster = labels[1:]\n",
    "to_cluster = np.append(to_cluster, [labels[-1]])\n",
    "len(to_cluster)\n",
    "data['Next Cluster'] = to_cluster\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Next Cluster'], axis = 1)\n",
    "y = data['Next Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414 104 414 104\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>Next Cluster</td>   <th>  R-squared (uncentered):</th>      <td>   0.793</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.787</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   148.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 01 Apr 2020</td> <th>  Prob (F-statistic):</th>          <td>4.91e-163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:39:18</td>     <th>  Log-Likelihood:    </th>          <td> -1078.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   518</td>      <th>  AIC:               </th>          <td>   2183.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   505</td>      <th>  BIC:               </th>          <td>   2238.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>               <td>   -4.4242</td> <td>    3.840</td> <td>   -1.152</td> <td> 0.250</td> <td>  -11.968</td> <td>    3.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>               <td>   -8.1067</td> <td>    5.835</td> <td>   -1.389</td> <td> 0.165</td> <td>  -19.571</td> <td>    3.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>               <td>   -1.0816</td> <td>    1.384</td> <td>   -0.781</td> <td> 0.435</td> <td>   -3.801</td> <td>    1.638</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>               <td>    3.9799</td> <td>    1.363</td> <td>    2.919</td> <td> 0.004</td> <td>    1.301</td> <td>    6.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>               <td>   -1.8754</td> <td>    0.831</td> <td>   -2.258</td> <td> 0.024</td> <td>   -3.507</td> <td>   -0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>               <td>   10.1838</td> <td>    4.471</td> <td>    2.278</td> <td> 0.023</td> <td>    1.399</td> <td>   18.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>               <td>    0.6425</td> <td>    1.621</td> <td>    0.396</td> <td> 0.692</td> <td>   -2.542</td> <td>    3.827</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>               <td>   -0.2688</td> <td>    1.439</td> <td>   -0.187</td> <td> 0.852</td> <td>   -3.097</td> <td>    2.559</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>               <td>   -0.5883</td> <td>    0.315</td> <td>   -1.869</td> <td> 0.062</td> <td>   -1.207</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>               <td>    1.1690</td> <td>    0.405</td> <td>    2.888</td> <td> 0.004</td> <td>    0.374</td> <td>    1.964</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>              <td>   -0.6005</td> <td>    0.310</td> <td>   -1.940</td> <td> 0.053</td> <td>   -1.209</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>              <td>    5.2895</td> <td>    0.583</td> <td>    9.080</td> <td> 0.000</td> <td>    4.145</td> <td>    6.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Current Cluster</th> <td>    0.5739</td> <td>    0.036</td> <td>   15.795</td> <td> 0.000</td> <td>    0.502</td> <td>    0.645</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>26.180</td> <th>  Durbin-Watson:     </th> <td>   2.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  48.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.326</td> <th>  Prob(JB):          </th> <td>3.45e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.344</td> <th>  Cond. No.          </th> <td>    402.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:           Next Cluster   R-squared (uncentered):                   0.793\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.787\n",
       "Method:                 Least Squares   F-statistic:                              148.5\n",
       "Date:                Wed, 01 Apr 2020   Prob (F-statistic):                   4.91e-163\n",
       "Time:                        19:39:18   Log-Likelihood:                         -1078.6\n",
       "No. Observations:                 518   AIC:                                      2183.\n",
       "Df Residuals:                     505   BIC:                                      2238.\n",
       "Df Model:                          13                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "0                  -4.4242      3.840     -1.152      0.250     -11.968       3.120\n",
       "1                  -8.1067      5.835     -1.389      0.165     -19.571       3.358\n",
       "2                  -1.0816      1.384     -0.781      0.435      -3.801       1.638\n",
       "3                   3.9799      1.363      2.919      0.004       1.301       6.659\n",
       "4                  -1.8754      0.831     -2.258      0.024      -3.507      -0.244\n",
       "5                  10.1838      4.471      2.278      0.023       1.399      18.968\n",
       "6                   0.6425      1.621      0.396      0.692      -2.542       3.827\n",
       "7                  -0.2688      1.439     -0.187      0.852      -3.097       2.559\n",
       "8                  -0.5883      0.315     -1.869      0.062      -1.207       0.030\n",
       "9                   1.1690      0.405      2.888      0.004       0.374       1.964\n",
       "10                 -0.6005      0.310     -1.940      0.053      -1.209       0.008\n",
       "11                  5.2895      0.583      9.080      0.000       4.145       6.434\n",
       "Current Cluster     0.5739      0.036     15.795      0.000       0.502       0.645\n",
       "==============================================================================\n",
       "Omnibus:                       26.180   Durbin-Watson:                   2.098\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               48.179\n",
       "Skew:                           0.326   Prob(JB):                     3.45e-11\n",
       "Kurtosis:                       4.344   Cond. No.                         402.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) # make the predictions by the model\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvergenceMonitor(\n",
       "    history=[4867.188489574209, 1483.2304139827618],\n",
       "    iter=6,\n",
       "    n_iter=100,\n",
       "    tol=0.01,\n",
       "    verbose=False,\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hmmlearn import hmm\n",
    "\n",
    "np.random.seed(42)\n",
    "X = data[['Current Cluster', 'Next Cluster']]\n",
    "model = hmm.GaussianHMM(n_components=10, covariance_type=\"full\", n_iter=100)\n",
    "model.fit(X)\n",
    "model.score(X)\n",
    "model.monitor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 9, 4, 4, 4, 4, 9, 4, 4, 4, 4, 4, 4, 4, 4, 6, 2, 4, 4, 4, 4, 9,\n",
       "       4, 6, 2, 4, 4, 4, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 6, 2, 6, 7, 7, 7,\n",
       "       2, 6, 7, 2, 6, 7, 2, 6, 7, 7, 7, 7, 7, 7, 7, 7, 2, 4, 6, 7, 7, 2,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4,\n",
       "       4, 4, 4, 4, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 6, 2, 4, 9, 9, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0,\n",
       "       9, 9, 4, 4, 4, 4, 9, 4, 9, 4, 4, 9, 9, 4, 4, 4, 4, 4, 9, 9, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 3, 7, 7,\n",
       "       8, 3, 7, 8, 4, 3, 7, 7, 7, 8, 3, 7, 8, 4, 9, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 3,\n",
       "       8, 4, 4, 4, 3, 7, 7, 7, 7, 5, 7, 8, 4, 4, 4, 3, 7, 7, 7, 7, 7, 7,\n",
       "       5, 7, 8, 4, 4, 4, 3, 7, 7, 7, 8, 4, 3, 6, 7, 8, 4, 4, 3, 2, 4, 4,\n",
       "       4, 4, 9, 6, 7, 2, 4, 6, 2, 4, 4, 6, 2, 4, 4, 4, 6, 7, 7, 2, 4, 4,\n",
       "       6, 7, 7, 2, 4, 6, 2, 4, 4, 4, 9, 6, 7, 2, 4, 4, 9, 4, 4, 4, 6, 7,\n",
       "       7, 2, 4, 9, 4, 4, 4, 6, 7, 2, 4, 6, 2, 6, 7, 7, 2, 6, 2, 4, 6, 7,\n",
       "       7, 7, 2, 6, 2, 4, 4, 6, 7, 7, 7, 2, 6, 2, 4, 4, 4, 9, 6, 7, 2, 4,\n",
       "       6, 2, 4, 6, 2, 4, 4, 4, 6, 7, 7, 2, 4, 9, 4, 4, 4, 6, 7, 7, 2, 4,\n",
       "       6, 2, 6, 7, 2, 4, 4, 9, 4, 4, 4, 4, 4, 6, 7, 7, 7, 2, 4, 6, 2, 6,\n",
       "       7, 7, 2, 4, 9, 4, 4, 4, 4, 6, 7, 7, 2, 4, 4, 9, 4, 4, 4, 9, 4, 4,\n",
       "       4, 4, 9, 4, 4, 4, 9, 4, 4, 4, 4, 4, 9, 9, 4, 4, 9, 4, 4, 9, 4, 4,\n",
       "       4, 4, 4, 4, 4, 9, 9, 9, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Current Cluster</th>\n",
       "      <th>Next Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Current Cluster  Next Cluster\n",
       "0                  6             6\n",
       "1                  6             5\n",
       "2                  5             5\n",
       "3                  5             5\n",
       "4                  5             5\n",
       "..               ...           ...\n",
       "513                7             4\n",
       "514                4             4\n",
       "515                4             4\n",
       "516                4             4\n",
       "517                4             4\n",
       "\n",
       "[518 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466 52 466 52\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "ncols = X_train.shape[1]\n",
    "\n",
    "model.add(Dense(10, activation = 'relu', input_shape = (ncols,)))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='Adamax',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 466 samples, validate on 52 samples\n",
      "Epoch 1/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.5954 - accuracy: 0.6030 - val_loss: 0.4918 - val_accuracy: 0.5000\n",
      "Epoch 2/40\n",
      "466/466 [==============================] - 0s 26us/step - loss: 0.5428 - accuracy: 0.6030 - val_loss: 0.4429 - val_accuracy: 0.5000\n",
      "Epoch 3/40\n",
      "466/466 [==============================] - 0s 31us/step - loss: 0.4913 - accuracy: 0.6030 - val_loss: 0.3985 - val_accuracy: 0.5000\n",
      "Epoch 4/40\n",
      "466/466 [==============================] - 0s 34us/step - loss: 0.4418 - accuracy: 0.6030 - val_loss: 0.3604 - val_accuracy: 0.5000\n",
      "Epoch 5/40\n",
      "466/466 [==============================] - 0s 32us/step - loss: 0.4003 - accuracy: 0.6330 - val_loss: 0.3254 - val_accuracy: 0.5000\n",
      "Epoch 6/40\n",
      "466/466 [==============================] - 0s 36us/step - loss: 0.3636 - accuracy: 0.6974 - val_loss: 0.2949 - val_accuracy: 0.6731\n",
      "Epoch 7/40\n",
      "466/466 [==============================] - 0s 31us/step - loss: 0.3299 - accuracy: 0.7275 - val_loss: 0.2651 - val_accuracy: 0.7692\n",
      "Epoch 8/40\n",
      "466/466 [==============================] - 0s 31us/step - loss: 0.2993 - accuracy: 0.7597 - val_loss: 0.2383 - val_accuracy: 0.7692\n",
      "Epoch 9/40\n",
      "466/466 [==============================] - 0s 33us/step - loss: 0.2693 - accuracy: 0.7639 - val_loss: 0.2154 - val_accuracy: 0.7692\n",
      "Epoch 10/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.2437 - accuracy: 0.7876 - val_loss: 0.1954 - val_accuracy: 0.8462\n",
      "Epoch 11/40\n",
      "466/466 [==============================] - 0s 29us/step - loss: 0.2205 - accuracy: 0.8369 - val_loss: 0.1754 - val_accuracy: 0.8846\n",
      "Epoch 12/40\n",
      "466/466 [==============================] - 0s 29us/step - loss: 0.2002 - accuracy: 0.8455 - val_loss: 0.1574 - val_accuracy: 0.9038\n",
      "Epoch 13/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.1803 - accuracy: 0.8755 - val_loss: 0.1425 - val_accuracy: 0.9038\n",
      "Epoch 14/40\n",
      "466/466 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.81 - 0s 29us/step - loss: 0.1635 - accuracy: 0.8755 - val_loss: 0.1289 - val_accuracy: 0.9038\n",
      "Epoch 15/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.1483 - accuracy: 0.8820 - val_loss: 0.1171 - val_accuracy: 0.9038\n",
      "Epoch 16/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.1356 - accuracy: 0.9185 - val_loss: 0.1059 - val_accuracy: 0.9038\n",
      "Epoch 17/40\n",
      "466/466 [==============================] - 0s 29us/step - loss: 0.1240 - accuracy: 0.9185 - val_loss: 0.0955 - val_accuracy: 0.9038\n",
      "Epoch 18/40\n",
      "466/466 [==============================] - 0s 29us/step - loss: 0.1129 - accuracy: 0.9185 - val_loss: 0.0868 - val_accuracy: 0.9038\n",
      "Epoch 19/40\n",
      "466/466 [==============================] - 0s 27us/step - loss: 0.1037 - accuracy: 0.9185 - val_loss: 0.0787 - val_accuracy: 0.9038\n",
      "Epoch 20/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.0950 - accuracy: 0.9185 - val_loss: 0.0716 - val_accuracy: 0.9038\n",
      "Epoch 21/40\n",
      "466/466 [==============================] - 0s 37us/step - loss: 0.0873 - accuracy: 0.9185 - val_loss: 0.0657 - val_accuracy: 0.9038\n",
      "Epoch 22/40\n",
      "466/466 [==============================] - 0s 29us/step - loss: 0.0809 - accuracy: 0.9442 - val_loss: 0.0599 - val_accuracy: 0.9808\n",
      "Epoch 23/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.0749 - accuracy: 0.9571 - val_loss: 0.0549 - val_accuracy: 0.9808\n",
      "Epoch 24/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.0693 - accuracy: 0.9571 - val_loss: 0.0504 - val_accuracy: 0.9808\n",
      "Epoch 25/40\n",
      "466/466 [==============================] - 0s 32us/step - loss: 0.0643 - accuracy: 0.9571 - val_loss: 0.0467 - val_accuracy: 0.9808\n",
      "Epoch 26/40\n",
      "466/466 [==============================] - 0s 32us/step - loss: 0.0600 - accuracy: 0.9571 - val_loss: 0.0431 - val_accuracy: 0.9808\n",
      "Epoch 27/40\n",
      "466/466 [==============================] - 0s 28us/step - loss: 0.0560 - accuracy: 0.9828 - val_loss: 0.0401 - val_accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "466/466 [==============================] - 0s 28us/step - loss: 0.0525 - accuracy: 0.9828 - val_loss: 0.0374 - val_accuracy: 0.9808\n",
      "Epoch 29/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.0494 - accuracy: 0.9764 - val_loss: 0.0347 - val_accuracy: 0.9808\n",
      "Epoch 30/40\n",
      "466/466 [==============================] - 0s 33us/step - loss: 0.0462 - accuracy: 0.9764 - val_loss: 0.0325 - val_accuracy: 0.9808\n",
      "Epoch 31/40\n",
      "466/466 [==============================] - 0s 31us/step - loss: 0.0436 - accuracy: 0.9764 - val_loss: 0.0306 - val_accuracy: 0.9808\n",
      "Epoch 32/40\n",
      "466/466 [==============================] - 0s 29us/step - loss: 0.0411 - accuracy: 0.9764 - val_loss: 0.0288 - val_accuracy: 0.9808\n",
      "Epoch 33/40\n",
      "466/466 [==============================] - 0s 27us/step - loss: 0.0389 - accuracy: 0.9764 - val_loss: 0.0273 - val_accuracy: 0.9808\n",
      "Epoch 34/40\n",
      "466/466 [==============================] - 0s 31us/step - loss: 0.0370 - accuracy: 0.9764 - val_loss: 0.0258 - val_accuracy: 0.9808\n",
      "Epoch 35/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.0352 - accuracy: 0.9764 - val_loss: 0.0245 - val_accuracy: 0.9808\n",
      "Epoch 36/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.0335 - accuracy: 0.9764 - val_loss: 0.0234 - val_accuracy: 0.9808\n",
      "Epoch 37/40\n",
      "466/466 [==============================] - 0s 32us/step - loss: 0.0320 - accuracy: 0.9764 - val_loss: 0.0224 - val_accuracy: 0.9808\n",
      "Epoch 38/40\n",
      "466/466 [==============================] - 0s 30us/step - loss: 0.0306 - accuracy: 0.9764 - val_loss: 0.0214 - val_accuracy: 0.9808\n",
      "Epoch 39/40\n",
      "466/466 [==============================] - 0s 32us/step - loss: 0.0294 - accuracy: 0.9785 - val_loss: 0.0205 - val_accuracy: 0.9808\n",
      "Epoch 40/40\n",
      "466/466 [==============================] - 0s 33us/step - loss: 0.0282 - accuracy: 0.9785 - val_loss: 0.0198 - val_accuracy: 0.9808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f23e43c4588>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=40, validation_data=(X_test, y_test), callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Predictions'] = np.rint(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>Current Cluster</th>\n",
       "      <th>Next Cluster</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052351</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>0.007387</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122578</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.184414</td>\n",
       "      <td>0.199815</td>\n",
       "      <td>0.295166</td>\n",
       "      <td>0.410972</td>\n",
       "      <td>0.415218</td>\n",
       "      <td>0.164890</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417736</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.149674</td>\n",
       "      <td>0.164176</td>\n",
       "      <td>0.194539</td>\n",
       "      <td>0.401805</td>\n",
       "      <td>0.534092</td>\n",
       "      <td>0.143095</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389543</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.290563</td>\n",
       "      <td>0.291288</td>\n",
       "      <td>0.529605</td>\n",
       "      <td>0.725636</td>\n",
       "      <td>0.692073</td>\n",
       "      <td>0.256825</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385361</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.214896</td>\n",
       "      <td>0.211615</td>\n",
       "      <td>0.190894</td>\n",
       "      <td>0.136036</td>\n",
       "      <td>0.272686</td>\n",
       "      <td>0.119368</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.656963</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0.056615</td>\n",
       "      <td>0.059945</td>\n",
       "      <td>0.060155</td>\n",
       "      <td>0.040176</td>\n",
       "      <td>0.125481</td>\n",
       "      <td>0.032763</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.660491</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.006295</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.188830</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>0.049597</td>\n",
       "      <td>0.051149</td>\n",
       "      <td>0.121193</td>\n",
       "      <td>0.099192</td>\n",
       "      <td>0.051149</td>\n",
       "      <td>0.042415</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.725830</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.406297</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>0.116715</td>\n",
       "      <td>0.099770</td>\n",
       "      <td>0.048171</td>\n",
       "      <td>0.081230</td>\n",
       "      <td>0.181439</td>\n",
       "      <td>0.076565</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.725830</td>\n",
       "      <td>0.957532</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.452888</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.000492  0.000480  0.002895  0.006325  0.007090  0.001951  0.000051   \n",
       "1    0.001699  0.001526  0.003426  0.004929  0.007387  0.002809  0.000043   \n",
       "2    0.184414  0.199815  0.295166  0.410972  0.415218  0.164890  0.000997   \n",
       "3    0.149674  0.164176  0.194539  0.401805  0.534092  0.143095  0.001172   \n",
       "4    0.290563  0.291288  0.529605  0.725636  0.692073  0.256825  0.001425   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "513  0.214896  0.211615  0.190894  0.136036  0.272686  0.119368  0.000312   \n",
       "514  0.056615  0.059945  0.060155  0.040176  0.125481  0.032763  0.000098   \n",
       "515  0.002819  0.002917  0.006295  0.007061  0.005291  0.003840  0.000523   \n",
       "516  0.049597  0.051149  0.121193  0.099192  0.051149  0.042415  0.001036   \n",
       "517  0.116715  0.099770  0.048171  0.081230  0.181439  0.076565  0.001124   \n",
       "\n",
       "            7         8         9        10        11  Current Cluster  \\\n",
       "0    0.000000  0.000000  0.000000  0.000000  0.052351                6   \n",
       "1    0.000013  0.000000  0.000000  0.000000  0.122578                6   \n",
       "2    0.000877  0.012987  0.000000  0.000000  0.417736                5   \n",
       "3    0.000990  0.066378  0.000000  0.000000  0.389543                5   \n",
       "4    0.001473  0.066378  0.000000  0.000000  0.385361                5   \n",
       "..        ...       ...       ...       ...       ...              ...   \n",
       "513  0.000243  0.722944  0.957532  0.096714  0.656963                7   \n",
       "514  0.001036  0.727273  0.957532  0.096714  0.660491                4   \n",
       "515  0.001621  0.727273  0.957532  0.096714  0.188830                4   \n",
       "516  0.001726  0.725830  0.957532  0.096714  0.406297                4   \n",
       "517  0.000139  0.725830  0.957532  0.096714  0.452888                4   \n",
       "\n",
       "     Next Cluster  Predictions  \n",
       "0               6          6.0  \n",
       "1               5          5.0  \n",
       "2               5          5.0  \n",
       "3               5          5.0  \n",
       "4               5          5.0  \n",
       "..            ...          ...  \n",
       "513             4          3.0  \n",
       "514             4          4.0  \n",
       "515             4          4.0  \n",
       "516             4          4.0  \n",
       "517             4          4.0  \n",
       "\n",
       "[518 rows x 15 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
